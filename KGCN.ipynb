{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KGCN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2hVxe4qDU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    n_user, n_item, train_data, eval_data, test_data = load_rating(args)\n",
        "    n_entity, n_relation, adj_entity, adj_relation = load_kg(args)\n",
        "    print('data loaded.')\n",
        "\n",
        "    return n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, adj_entity, adj_relation\n",
        "\n",
        "\n",
        "def load_rating(args):\n",
        "    print('reading rating file ...')\n",
        "\n",
        "    # reading rating file\n",
        "    rating_file = '../data/' + args.dataset + '/ratings_final'\n",
        "    if os.path.exists(rating_file + '.npy'):\n",
        "        rating_np = np.load(rating_file + '.npy')\n",
        "    else:\n",
        "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int64)\n",
        "        np.save(rating_file + '.npy', rating_np)\n",
        "\n",
        "    n_user = len(set(rating_np[:, 0]))\n",
        "    n_item = len(set(rating_np[:, 1]))\n",
        "    train_data, eval_data, test_data = dataset_split(rating_np, args)\n",
        "\n",
        "    return n_user, n_item, train_data, eval_data, test_data\n",
        "\n",
        "\n",
        "def dataset_split(rating_np, args):\n",
        "    print('splitting dataset ...')\n",
        "\n",
        "    # train:eval:test = 6:2:2\n",
        "    eval_ratio = 0.2\n",
        "    test_ratio = 0.2\n",
        "    n_ratings = rating_np.shape[0]\n",
        "\n",
        "    eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
        "    left = set(range(n_ratings)) - set(eval_indices)\n",
        "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
        "    train_indices = list(left - set(test_indices))\n",
        "    if args.ratio < 1:\n",
        "        train_indices = np.random.choice(list(train_indices), size=int(len(train_indices) * args.ratio), replace=False)\n",
        "\n",
        "    train_data = rating_np[train_indices]\n",
        "    eval_data = rating_np[eval_indices]\n",
        "    test_data = rating_np[test_indices]\n",
        "\n",
        "    return train_data, eval_data, test_data\n",
        "\n",
        "\n",
        "def load_kg(args):\n",
        "    print('reading KG file ...')\n",
        "\n",
        "    # reading kg file\n",
        "    kg_file = '../data/' + args.dataset + '/kg_final'\n",
        "    if os.path.exists(kg_file + '.npy'):\n",
        "        kg_np = np.load(kg_file + '.npy')\n",
        "    else:\n",
        "        kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int64)\n",
        "        np.save(kg_file + '.npy', kg_np)\n",
        "\n",
        "    n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
        "    n_relation = len(set(kg_np[:, 1]))\n",
        "\n",
        "    kg = construct_kg(kg_np)\n",
        "    adj_entity, adj_relation = construct_adj(args, kg, n_entity)\n",
        "\n",
        "    return n_entity, n_relation, adj_entity, adj_relation\n",
        "\n",
        "\n",
        "def construct_kg(kg_np):\n",
        "    print('constructing knowledge graph ...')\n",
        "    kg = dict()\n",
        "    for triple in kg_np:\n",
        "        head = triple[0]\n",
        "        relation = triple[1]\n",
        "        tail = triple[2]\n",
        "        # treat the KG as an undirected graph\n",
        "        if head not in kg:\n",
        "            kg[head] = []\n",
        "        kg[head].append((tail, relation))\n",
        "        if tail not in kg:\n",
        "            kg[tail] = []\n",
        "        kg[tail].append((head, relation))\n",
        "    return kg\n",
        "\n",
        "\n",
        "def construct_adj(args, kg, entity_num):\n",
        "    print('constructing adjacency matrix ...')\n",
        "    # each line of adj_entity stores the sampled neighbor entities for a given entity\n",
        "    # each line of adj_relation stores the corresponding sampled neighbor relations\n",
        "    adj_entity = np.zeros([entity_num, args.neighbor_sample_size], dtype=np.int64)\n",
        "    adj_relation = np.zeros([entity_num, args.neighbor_sample_size], dtype=np.int64)\n",
        "    for entity in range(entity_num):\n",
        "        neighbors = kg[entity]\n",
        "        n_neighbors = len(neighbors)\n",
        "        if n_neighbors >= args.neighbor_sample_size:\n",
        "            sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.neighbor_sample_size, replace=False)\n",
        "        else:\n",
        "            sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.neighbor_sample_size, replace=True)\n",
        "        adj_entity[entity] = np.array([neighbors[i][0] for i in sampled_indices])\n",
        "        adj_relation[entity] = np.array([neighbors[i][1] for i in sampled_indices])\n",
        "\n",
        "    return adj_entity, adj_relation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_l7SuonzvvU",
        "colab_type": "code",
        "outputId": "88a15679-880c-4c62-e776-31f5439afc7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "RATING_FILE_NAME = dict({'movie': 'ratings.csv', 'book': 'BX-Book-Ratings.csv', 'music': 'user_artists.dat'})\n",
        "SEP = dict({'movie': ',', 'book': ';', 'music': '\\t'})\n",
        "THRESHOLD = dict({'movie': 4, 'book': 0, 'music': 0})\n",
        "\n",
        "\n",
        "def read_item_index_to_entity_id_file():\n",
        "    file = '../data/' + DATASET + '/item_index2entity_id.txt'\n",
        "    print('reading item index to entity id file: ' + file + ' ...')\n",
        "    i = 0\n",
        "    for line in open(file, encoding='utf-8').readlines():\n",
        "        item_index = line.strip().split('\\t')[0]\n",
        "        satori_id = line.strip().split('\\t')[1]\n",
        "        item_index_old2new[item_index] = i\n",
        "        entity_id2index[satori_id] = i\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def convert_rating():\n",
        "    file = '../data/' + DATASET + '/' + RATING_FILE_NAME[DATASET]\n",
        "\n",
        "    print('reading rating file ...')\n",
        "    item_set = set(item_index_old2new.values())\n",
        "    user_pos_ratings = dict()\n",
        "    user_neg_ratings = dict()\n",
        "\n",
        "    for line in open(file, encoding='utf-8').readlines()[1:]:\n",
        "        array = line.strip().split(SEP[DATASET])\n",
        "\n",
        "        # remove prefix and suffix quotation marks for BX dataset\n",
        "        if DATASET == 'book':\n",
        "            array = list(map(lambda x: x[1:-1], array))\n",
        "\n",
        "        item_index_old = array[1]\n",
        "        if item_index_old not in item_index_old2new:  # the item is not in the final item set\n",
        "            continue\n",
        "        item_index = item_index_old2new[item_index_old]\n",
        "\n",
        "        user_index_old = int(array[0])\n",
        "\n",
        "        rating = float(array[2])\n",
        "        if rating >= THRESHOLD[DATASET]:\n",
        "            if user_index_old not in user_pos_ratings:\n",
        "                user_pos_ratings[user_index_old] = set()\n",
        "            user_pos_ratings[user_index_old].add(item_index)\n",
        "        else:\n",
        "            if user_index_old not in user_neg_ratings:\n",
        "                user_neg_ratings[user_index_old] = set()\n",
        "            user_neg_ratings[user_index_old].add(item_index)\n",
        "\n",
        "    print('converting rating file ...')\n",
        "    writer = open('../data/' + DATASET + '/ratings_final.txt', 'w', encoding='utf-8')\n",
        "    user_cnt = 0\n",
        "    user_index_old2new = dict()\n",
        "    for user_index_old, pos_item_set in user_pos_ratings.items():\n",
        "        if user_index_old not in user_index_old2new:\n",
        "            user_index_old2new[user_index_old] = user_cnt\n",
        "            user_cnt += 1\n",
        "        user_index = user_index_old2new[user_index_old]\n",
        "\n",
        "        for item in pos_item_set:\n",
        "            writer.write('%d\\t%d\\t1\\n' % (user_index, item))\n",
        "        unwatched_set = item_set - pos_item_set\n",
        "        if user_index_old in user_neg_ratings:\n",
        "            unwatched_set -= user_neg_ratings[user_index_old]\n",
        "        for item in np.random.choice(list(unwatched_set), size=len(pos_item_set), replace=False):\n",
        "            writer.write('%d\\t%d\\t0\\n' % (user_index, item))\n",
        "    writer.close()\n",
        "    print('number of users: %d' % user_cnt)\n",
        "    print('number of items: %d' % len(item_set))\n",
        "\n",
        "\n",
        "def convert_kg():\n",
        "    print('converting kg file ...')\n",
        "    entity_cnt = len(entity_id2index)\n",
        "    relation_cnt = 0\n",
        "\n",
        "    writer = open('../data/' + DATASET + '/kg_final.txt', 'w', encoding='utf-8')\n",
        "    for line in open('../data/' + DATASET + '/kg.txt', encoding='utf-8'):\n",
        "        array = line.strip().split('\\t')\n",
        "        head_old = array[0]\n",
        "        relation_old = array[1]\n",
        "        tail_old = array[2]\n",
        "\n",
        "        if head_old not in entity_id2index:\n",
        "            entity_id2index[head_old] = entity_cnt\n",
        "            entity_cnt += 1\n",
        "        head = entity_id2index[head_old]\n",
        "\n",
        "        if tail_old not in entity_id2index:\n",
        "            entity_id2index[tail_old] = entity_cnt\n",
        "            entity_cnt += 1\n",
        "        tail = entity_id2index[tail_old]\n",
        "\n",
        "        if relation_old not in relation_id2index:\n",
        "            relation_id2index[relation_old] = relation_cnt\n",
        "            relation_cnt += 1\n",
        "        relation = relation_id2index[relation_old]\n",
        "\n",
        "        writer.write('%d\\t%d\\t%d\\n' % (head, relation, tail))\n",
        "\n",
        "    writer.close()\n",
        "    print('number of entities (containing items): %d' % entity_cnt)\n",
        "    print('number of relations: %d' % relation_cnt)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(555)\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-d', type=str, default='movie', help='which dataset to preprocess')\n",
        "    args = parser.parse_args()\n",
        "    DATASET = args.d\n",
        "\n",
        "    entity_id2index = dict()\n",
        "    relation_id2index = dict()\n",
        "    item_index_old2new = dict()\n",
        "\n",
        "    read_item_index_to_entity_id_file()\n",
        "    convert_rating()\n",
        "    convert_kg()\n",
        "\n",
        "    print('done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-d D]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-d55a90ff-23c6-41ab-920b-15fe00190ef4.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UNAr0v07ps5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from aggregators import SumAggregator, ConcatAggregator, NeighborAggregator\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "class KGCN(object):\n",
        "    def __init__(self, args, n_user, n_entity, n_relation, adj_entity, adj_relation):\n",
        "        self._parse_args(args, adj_entity, adj_relation)\n",
        "        self._build_inputs()\n",
        "        self._build_model(n_user, n_entity, n_relation)\n",
        "        self._build_train()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_initializer():\n",
        "        return tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    def _parse_args(self, args, adj_entity, adj_relation):\n",
        "        # [entity_num, neighbor_sample_size]\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "\n",
        "        self.n_iter = args.n_iter\n",
        "        self.batch_size = args.batch_size\n",
        "        self.n_neighbor = args.neighbor_sample_size\n",
        "        self.dim = args.dim\n",
        "        self.l2_weight = args.l2_weight\n",
        "        self.lr = args.lr\n",
        "        if args.aggregator == 'sum':\n",
        "            self.aggregator_class = SumAggregator\n",
        "        elif args.aggregator == 'concat':\n",
        "            self.aggregator_class = ConcatAggregator\n",
        "        elif args.aggregator == 'neighbor':\n",
        "            self.aggregator_class = NeighborAggregator\n",
        "        else:\n",
        "            raise Exception(\"Unknown aggregator: \" + args.aggregator)\n",
        "\n",
        "    def _build_inputs(self):\n",
        "        self.user_indices = tf.placeholder(dtype=tf.int64, shape=[None], name='user_indices')\n",
        "        self.item_indices = tf.placeholder(dtype=tf.int64, shape=[None], name='item_indices')\n",
        "        self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
        "\n",
        "    def _build_model(self, n_user, n_entity, n_relation):\n",
        "        self.user_emb_matrix = tf.get_variable(\n",
        "            shape=[n_user, self.dim], initializer=KGCN.get_initializer(), name='user_emb_matrix')\n",
        "        self.entity_emb_matrix = tf.get_variable(\n",
        "            shape=[n_entity, self.dim], initializer=KGCN.get_initializer(), name='entity_emb_matrix')\n",
        "        self.relation_emb_matrix = tf.get_variable(\n",
        "            shape=[n_relation, self.dim], initializer=KGCN.get_initializer(), name='relation_emb_matrix')\n",
        "\n",
        "        # [batch_size, dim]\n",
        "        self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)\n",
        "\n",
        "        # entities is a list of i-iter (i = 0, 1, ..., n_iter) neighbors for the batch of items\n",
        "        # dimensions of entities:\n",
        "        # {[batch_size, 1], [batch_size, n_neighbor], [batch_size, n_neighbor^2], ..., [batch_size, n_neighbor^n_iter]}\n",
        "        entities, relations = self.get_neighbors(self.item_indices)\n",
        "\n",
        "        # [batch_size, dim]\n",
        "        self.item_embeddings, self.aggregators = self.aggregate(entities, relations)\n",
        "\n",
        "        # [batch_size]\n",
        "        self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)\n",
        "        self.scores_normalized = tf.sigmoid(self.scores)\n",
        "\n",
        "    def get_neighbors(self, seeds):\n",
        "        seeds = tf.expand_dims(seeds, axis=1)\n",
        "        entities = [seeds]\n",
        "        relations = []\n",
        "        for i in range(self.n_iter):\n",
        "            neighbor_entities = tf.reshape(tf.gather(self.adj_entity, entities[i]), [self.batch_size, -1])\n",
        "            neighbor_relations = tf.reshape(tf.gather(self.adj_relation, entities[i]), [self.batch_size, -1])\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations\n",
        "\n",
        "    def aggregate(self, entities, relations):\n",
        "        aggregators = []  # store all aggregators\n",
        "        entity_vectors = [tf.nn.embedding_lookup(self.entity_emb_matrix, i) for i in entities]\n",
        "        relation_vectors = [tf.nn.embedding_lookup(self.relation_emb_matrix, i) for i in relations]\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            if i == self.n_iter - 1:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.dim, act=tf.nn.tanh)\n",
        "            else:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.dim)\n",
        "            aggregators.append(aggregator)\n",
        "\n",
        "            entity_vectors_next_iter = []\n",
        "            for hop in range(self.n_iter - i):\n",
        "                shape = [self.batch_size, -1, self.n_neighbor, self.dim]\n",
        "                vector = aggregator(self_vectors=entity_vectors[hop],\n",
        "                                    neighbor_vectors=tf.reshape(entity_vectors[hop + 1], shape),\n",
        "                                    neighbor_relations=tf.reshape(relation_vectors[hop], shape),\n",
        "                                    user_embeddings=self.user_embeddings)\n",
        "                entity_vectors_next_iter.append(vector)\n",
        "            entity_vectors = entity_vectors_next_iter\n",
        "\n",
        "        res = tf.reshape(entity_vectors[0], [self.batch_size, self.dim])\n",
        "\n",
        "        return res, aggregators\n",
        "\n",
        "    def _build_train(self):\n",
        "        self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=self.labels, logits=self.scores))\n",
        "\n",
        "        self.l2_loss = tf.nn.l2_loss(self.user_emb_matrix) + tf.nn.l2_loss(\n",
        "            self.entity_emb_matrix) + tf.nn.l2_loss(self.relation_emb_matrix)\n",
        "        for aggregator in self.aggregators:\n",
        "            self.l2_loss = self.l2_loss + tf.nn.l2_loss(aggregator.weights)\n",
        "        self.loss = self.base_loss + self.l2_weight * self.l2_loss\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
        "\n",
        "    def train(self, sess, feed_dict):\n",
        "        return sess.run([self.optimizer, self.loss], feed_dict)\n",
        "\n",
        "    def eval(self, sess, feed_dict):\n",
        "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
        "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
        "        scores[scores >= 0.5] = 1\n",
        "        scores[scores < 0.5] = 0\n",
        "        f1 = f1_score(y_true=labels, y_pred=scores)\n",
        "        return auc, f1\n",
        "\n",
        "    def get_scores(self, sess, feed_dict):\n",
        "        return sess.run([self.item_indices, self.scores_normalized], feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W8hiF_09EeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "from time import time\n",
        "from data_loader import load_data\n",
        "from train import train\n",
        "\n",
        "np.random.seed(555)\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# movie\n",
        "parser.add_argument('--dataset', type=str, default='movie', help='which dataset to use')\n",
        "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
        "parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
        "parser.add_argument('--neighbor_sample_size', type=int, default=4, help='the number of neighbors to be sampled')\n",
        "parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')\n",
        "parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')\n",
        "parser.add_argument('--batch_size', type=int, default=65536, help='batch size')\n",
        "parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')\n",
        "parser.add_argument('--lr', type=float, default=2e-2, help='learning rate')\n",
        "parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')\n",
        "\n",
        "'''\n",
        "# book\n",
        "parser.add_argument('--dataset', type=str, default='book', help='which dataset to use')\n",
        "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
        "parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
        "parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
        "parser.add_argument('--dim', type=int, default=64, help='dimension of user and entity embeddings')\n",
        "parser.add_argument('--n_iter', type=int, default=3, help='number of iterations when computing entity representation')\n",
        "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
        "parser.add_argument('--l2_weight', type=float, default=2e-5, help='weight of l2 regularization')\n",
        "parser.add_argument('--lr', type=float, default=2e-4, help='learning rate')\n",
        "parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')\n",
        "'''\n",
        "\n",
        "'''\n",
        "# music\n",
        "parser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\n",
        "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
        "parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
        "parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
        "parser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')\n",
        "parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
        "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
        "parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
        "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
        "parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')\n",
        "'''\n",
        "\n",
        "\n",
        "show_loss = False\n",
        "show_time = False\n",
        "show_topk = False\n",
        "\n",
        "t = time()\n",
        "\n",
        "args = parser.parse_args()\n",
        "data = load_data(args)\n",
        "train(args, data, show_loss, show_topk)\n",
        "\n",
        "if show_time:\n",
        "    print('time used: %d s' % (time() - t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3xeRrVJ8tPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from aggregators import SumAggregator, ConcatAggregator, NeighborAggregator\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "class KGCN(object):\n",
        "    def __init__(self, args, n_user, n_entity, n_relation, adj_entity, adj_relation):\n",
        "        self._parse_args(args, adj_entity, adj_relation)\n",
        "        self._build_inputs()\n",
        "        self._build_model(n_user, n_entity, n_relation)\n",
        "        self._build_train()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_initializer():\n",
        "        return tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    def _parse_args(self, args, adj_entity, adj_relation):\n",
        "        # [entity_num, neighbor_sample_size]\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "\n",
        "        self.n_iter = args.n_iter\n",
        "        self.batch_size = args.batch_size\n",
        "        self.n_neighbor = args.neighbor_sample_size\n",
        "        self.dim = args.dim\n",
        "        self.l2_weight = args.l2_weight\n",
        "        self.lr = args.lr\n",
        "        if args.aggregator == 'sum':\n",
        "            self.aggregator_class = SumAggregator\n",
        "        elif args.aggregator == 'concat':\n",
        "            self.aggregator_class = ConcatAggregator\n",
        "        elif args.aggregator == 'neighbor':\n",
        "            self.aggregator_class = NeighborAggregator\n",
        "        else:\n",
        "            raise Exception(\"Unknown aggregator: \" + args.aggregator)\n",
        "\n",
        "    def _build_inputs(self):\n",
        "        self.user_indices = tf.placeholder(dtype=tf.int64, shape=[None], name='user_indices')\n",
        "        self.item_indices = tf.placeholder(dtype=tf.int64, shape=[None], name='item_indices')\n",
        "        self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
        "\n",
        "    def _build_model(self, n_user, n_entity, n_relation):\n",
        "        self.user_emb_matrix = tf.get_variable(\n",
        "            shape=[n_user, self.dim], initializer=KGCN.get_initializer(), name='user_emb_matrix')\n",
        "        self.entity_emb_matrix = tf.get_variable(\n",
        "            shape=[n_entity, self.dim], initializer=KGCN.get_initializer(), name='entity_emb_matrix')\n",
        "        self.relation_emb_matrix = tf.get_variable(\n",
        "            shape=[n_relation, self.dim], initializer=KGCN.get_initializer(), name='relation_emb_matrix')\n",
        "\n",
        "        # [batch_size, dim]\n",
        "        self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)\n",
        "\n",
        "        # entities is a list of i-iter (i = 0, 1, ..., n_iter) neighbors for the batch of items\n",
        "        # dimensions of entities:\n",
        "        # {[batch_size, 1], [batch_size, n_neighbor], [batch_size, n_neighbor^2], ..., [batch_size, n_neighbor^n_iter]}\n",
        "        entities, relations = self.get_neighbors(self.item_indices)\n",
        "\n",
        "        # [batch_size, dim]\n",
        "        self.item_embeddings, self.aggregators = self.aggregate(entities, relations)\n",
        "\n",
        "        # [batch_size]\n",
        "        self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)\n",
        "        self.scores_normalized = tf.sigmoid(self.scores)\n",
        "\n",
        "    def get_neighbors(self, seeds):\n",
        "        seeds = tf.expand_dims(seeds, axis=1)\n",
        "        entities = [seeds]\n",
        "        relations = []\n",
        "        for i in range(self.n_iter):\n",
        "            neighbor_entities = tf.reshape(tf.gather(self.adj_entity, entities[i]), [self.batch_size, -1])\n",
        "            neighbor_relations = tf.reshape(tf.gather(self.adj_relation, entities[i]), [self.batch_size, -1])\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations\n",
        "\n",
        "    def aggregate(self, entities, relations):\n",
        "        aggregators = []  # store all aggregators\n",
        "        entity_vectors = [tf.nn.embedding_lookup(self.entity_emb_matrix, i) for i in entities]\n",
        "        relation_vectors = [tf.nn.embedding_lookup(self.relation_emb_matrix, i) for i in relations]\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            if i == self.n_iter - 1:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.dim, act=tf.nn.tanh)\n",
        "            else:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.dim)\n",
        "            aggregators.append(aggregator)\n",
        "\n",
        "            entity_vectors_next_iter = []\n",
        "            for hop in range(self.n_iter - i):\n",
        "                shape = [self.batch_size, -1, self.n_neighbor, self.dim]\n",
        "                vector = aggregator(self_vectors=entity_vectors[hop],\n",
        "                                    neighbor_vectors=tf.reshape(entity_vectors[hop + 1], shape),\n",
        "                                    neighbor_relations=tf.reshape(relation_vectors[hop], shape),\n",
        "                                    user_embeddings=self.user_embeddings)\n",
        "                entity_vectors_next_iter.append(vector)\n",
        "            entity_vectors = entity_vectors_next_iter\n",
        "\n",
        "        res = tf.reshape(entity_vectors[0], [self.batch_size, self.dim])\n",
        "\n",
        "        return res, aggregators\n",
        "\n",
        "    def _build_train(self):\n",
        "        self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            labels=self.labels, logits=self.scores))\n",
        "\n",
        "        self.l2_loss = tf.nn.l2_loss(self.user_emb_matrix) + tf.nn.l2_loss(\n",
        "            self.entity_emb_matrix) + tf.nn.l2_loss(self.relation_emb_matrix)\n",
        "        for aggregator in self.aggregators:\n",
        "            self.l2_loss = self.l2_loss + tf.nn.l2_loss(aggregator.weights)\n",
        "        self.loss = self.base_loss + self.l2_weight * self.l2_loss\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
        "\n",
        "    def train(self, sess, feed_dict):\n",
        "        return sess.run([self.optimizer, self.loss], feed_dict)\n",
        "\n",
        "    def eval(self, sess, feed_dict):\n",
        "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
        "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
        "        scores[scores >= 0.5] = 1\n",
        "        scores[scores < 0.5] = 0\n",
        "        f1 = f1_score(y_true=labels, y_pred=scores)\n",
        "        return auc, f1\n",
        "\n",
        "    def get_scores(self, sess, feed_dict):\n",
        "        return sess.run([self.item_indices, self.scores_normalized], feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osHCg4uG9JTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from model import KGCN\n",
        "\n",
        "\n",
        "def train(args, data, show_loss, show_topk):\n",
        "    n_user, n_item, n_entity, n_relation = data[0], data[1], data[2], data[3]\n",
        "    train_data, eval_data, test_data = data[4], data[5], data[6]\n",
        "    adj_entity, adj_relation = data[7], data[8]\n",
        "\n",
        "    model = KGCN(args, n_user, n_entity, n_relation, adj_entity, adj_relation)\n",
        "\n",
        "    # top-K evaluation settings\n",
        "    user_list, train_record, test_record, item_set, k_list = topk_settings(show_topk, train_data, test_data, n_item)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        for step in range(args.n_epochs):\n",
        "            # training\n",
        "            np.random.shuffle(train_data)\n",
        "            start = 0\n",
        "            # skip the last incomplete minibatch if its size < batch size\n",
        "            while start + args.batch_size <= train_data.shape[0]:\n",
        "                _, loss = model.train(sess, get_feed_dict(model, train_data, start, start + args.batch_size))\n",
        "                start += args.batch_size\n",
        "                if show_loss:\n",
        "                    print(start, loss)\n",
        "\n",
        "            # CTR evaluation\n",
        "            train_auc, train_f1 = ctr_eval(sess, model, train_data, args.batch_size)\n",
        "            eval_auc, eval_f1 = ctr_eval(sess, model, eval_data, args.batch_size)\n",
        "            test_auc, test_f1 = ctr_eval(sess, model, test_data, args.batch_size)\n",
        "\n",
        "            print('epoch %d    train auc: %.4f  f1: %.4f    eval auc: %.4f  f1: %.4f    test auc: %.4f  f1: %.4f'\n",
        "                  % (step, train_auc, train_f1, eval_auc, eval_f1, test_auc, test_f1))\n",
        "\n",
        "            # top-K evaluation\n",
        "            if show_topk:\n",
        "                precision, recall = topk_eval(\n",
        "                    sess, model, user_list, train_record, test_record, item_set, k_list, args.batch_size)\n",
        "                print('precision: ', end='')\n",
        "                for i in precision:\n",
        "                    print('%.4f\\t' % i, end='')\n",
        "                print()\n",
        "                print('recall: ', end='')\n",
        "                for i in recall:\n",
        "                    print('%.4f\\t' % i, end='')\n",
        "                print('\\n')\n",
        "\n",
        "\n",
        "def topk_settings(show_topk, train_data, test_data, n_item):\n",
        "    if show_topk:\n",
        "        user_num = 100\n",
        "        k_list = [1, 2, 5, 10, 20, 50, 100]\n",
        "        train_record = get_user_record(train_data, True)\n",
        "        test_record = get_user_record(test_data, False)\n",
        "        user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
        "        if len(user_list) > user_num:\n",
        "            user_list = np.random.choice(user_list, size=user_num, replace=False)\n",
        "        item_set = set(list(range(n_item)))\n",
        "        return user_list, train_record, test_record, item_set, k_list\n",
        "    else:\n",
        "        return [None] * 5\n",
        "\n",
        "\n",
        "def get_feed_dict(model, data, start, end):\n",
        "    feed_dict = {model.user_indices: data[start:end, 0],\n",
        "                 model.item_indices: data[start:end, 1],\n",
        "                 model.labels: data[start:end, 2]}\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def ctr_eval(sess, model, data, batch_size):\n",
        "    start = 0\n",
        "    auc_list = []\n",
        "    f1_list = []\n",
        "    while start + batch_size <= data.shape[0]:\n",
        "        auc, f1 = model.eval(sess, get_feed_dict(model, data, start, start + batch_size))\n",
        "        auc_list.append(auc)\n",
        "        f1_list.append(f1)\n",
        "        start += batch_size\n",
        "    return float(np.mean(auc_list)), float(np.mean(f1_list))\n",
        "\n",
        "\n",
        "def topk_eval(sess, model, user_list, train_record, test_record, item_set, k_list, batch_size):\n",
        "    precision_list = {k: [] for k in k_list}\n",
        "    recall_list = {k: [] for k in k_list}\n",
        "\n",
        "    for user in user_list:\n",
        "        test_item_list = list(item_set - train_record[user])\n",
        "        item_score_map = dict()\n",
        "        start = 0\n",
        "        while start + batch_size <= len(test_item_list):\n",
        "            items, scores = model.get_scores(sess, {model.user_indices: [user] * batch_size,\n",
        "                                                    model.item_indices: test_item_list[start:start + batch_size]})\n",
        "            for item, score in zip(items, scores):\n",
        "                item_score_map[item] = score\n",
        "            start += batch_size\n",
        "\n",
        "        # padding the last incomplete minibatch if exists\n",
        "        if start < len(test_item_list):\n",
        "            items, scores = model.get_scores(\n",
        "                sess, {model.user_indices: [user] * batch_size,\n",
        "                       model.item_indices: test_item_list[start:] + [test_item_list[-1]] * (\n",
        "                               batch_size - len(test_item_list) + start)})\n",
        "            for item, score in zip(items, scores):\n",
        "                item_score_map[item] = score\n",
        "\n",
        "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
        "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
        "\n",
        "        for k in k_list:\n",
        "            hit_num = len(set(item_sorted[:k]) & test_record[user])\n",
        "            precision_list[k].append(hit_num / k)\n",
        "            recall_list[k].append(hit_num / len(test_record[user]))\n",
        "\n",
        "    precision = [np.mean(precision_list[k]) for k in k_list]\n",
        "    recall = [np.mean(recall_list[k]) for k in k_list]\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "def get_user_record(data, is_train):\n",
        "    user_history_dict = dict()\n",
        "    for interaction in data:\n",
        "        user = interaction[0]\n",
        "        item = interaction[1]\n",
        "        label = interaction[2]\n",
        "        if is_train or label == 1:\n",
        "            if user not in user_history_dict:\n",
        "                user_history_dict[user] = set()\n",
        "            user_history_dict[user].add(item)\n",
        "    return user_history_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKIpgnsd9Y4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}